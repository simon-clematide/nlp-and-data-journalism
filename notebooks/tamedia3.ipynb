{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic models with Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "unsupervised search for classes of terms (words) that form a topic given a particular text corpus\n",
    "\n",
    "methods: NMF and LDA (for comparison)\n",
    "\n",
    "use TuebaD/Z (German newspaper): only nouns are being used\n",
    "\n",
    "### Content\n",
    "\n",
    "* topic modelling of a newspaper corpus: small data set (first)\n",
    "     * vectorize and factorize the data\n",
    "* use the models\n",
    "    * similarity of documents\n",
    "    * similarity of words\n",
    "* topic modelling of a newspaper corpus: full data\n",
    "    * visualize and explore the model with pyLDAvis\n",
    "* extractive summarization using the topic model\n",
    "    * find the 5 most important sentences of a document given its topic\n",
    "    * find the most important speech given a topic\n",
    "\n",
    "\n",
    "### Explorative Setting\n",
    "\n",
    "* 6 documents (documents=documents[:6])\n",
    "* 10 words uses as features (no_features = 10)\n",
    "* number of topics: 4 (no_topics = 4)\n",
    "* number of words used to characterise topics: 5 (no_top_words=5)\n",
    "\n",
    "* M = 6x10, W=6x4, H=4x10\n",
    "* M=WxH, M = documents x words, W = documents x topics, H = topics x words\n",
    "* H = word space, W = document space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://files.ifi.uzh.ch/cl/siclemat/lehre/hs19/tm/lemmaNoun.tueba -O lemmaNoun.tueba\n",
    "! wget https://files.ifi.uzh.ch/cl/siclemat/lehre/hs19/tm/sp.tsv -O sp.tsv\n",
    "! wget https://files.ifi.uzh.ch/cl/siclemat/lehre/hs19/tm/svp.tsv -O svp.tsv\n",
    "! wget https://files.ifi.uzh.ch/cl/siclemat/lehre/hs19/tm/blocher.tsv -O blocher.tsv\n",
    "! wget https://files.ifi.uzh.ch/cl/siclemat/lehre/hs19/tm/doc.tueba -O doc.tueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the documents (list of lemmata from the TuebaD/Z)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "exec(open(\"lemmaNoun.tueba\").read())  # this is a file with: documents=['doc1',...]\n",
    "\n",
    "documents6=documents[:6]  # just 6 documents \n",
    "documents[0]\n",
    "\n",
    "def removeDocs(documents):\n",
    "    \"\"\" remove too small documents, helps pyLDAvis\"\"\"\n",
    "    doc=[]\n",
    "\n",
    "    for i in range(0,len(documents)-1):\n",
    "        if len(documents[i]) <250 :\n",
    "            continue\n",
    "        else:\n",
    "            doc.append(documents[i])\n",
    "\n",
    "    return doc\n",
    "\n",
    "documents=removeDocs(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 10\n",
    "\n",
    "# we get a matrix with tifidf cell entries: 6 rows (documents) and |V| columns (vocabulary)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, max_features=no_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents6)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max_df=skip words if they are in 95% of the documents\n",
    "* min_df=only words that occur 1 times or more\n",
    "* max_features= build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize\n",
    "\n",
    "no_topics = number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 4\n",
    "\n",
    "# Instantiate NMF, the number of columns of W will be 4, whereas H has 4 lines\n",
    "nmf = NMF(n_components=no_topics, solver='cd',beta_loss=2, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy argsort: sorts values (increasing) and return there index positions\n",
    "\n",
    "np.array([1.2,1.4,4.3]).argsort()  produces: array([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    \"\"\" each line of H is a topic, we get the indexes of the strongest weighted word and print them from feature_names\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit W, H comes for free\n",
    "\n",
    "W=nmf.fit_transform(tfidf)\n",
    "H=nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W  # documents space, no probabilities, just weights; document 1 has (best) topic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H # e.g. topic 0's most important term (0.755) is number 4  which is 'daewoo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words=5\n",
    "\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic of a document, similarity of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(X,H,no_topics):\n",
    "    \"\"\" which topic is most similar to X (a tfidf representation of a document)\"\"\"\n",
    "    bestsim=0\n",
    "    for i in range(0,no_topics):\n",
    "        sim=np.dot(X,H[i])/(norm(X)*norm(H[i]))\n",
    "        if sim>bestsim:\n",
    "            bestsim=sim\n",
    "            index=i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could (should) use a new document, but for simplicity, we use an existing on\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "X=tfidf[2].toarray()     # 3. document as tfidf \n",
    "Y=tfidf[3].toarray()     # 4. document\n",
    "\n",
    "t3=getTopic(X,H,no_topics)\n",
    "t4=getTopic(Y,H,no_topics)\n",
    "t3,t4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# which is the document closest to doc 2 based on the topic representation\n",
    "# just compare to the rows of W with cosine \n",
    "\n",
    "bestsim=0\n",
    "for i in range(0,len(documents6)):\n",
    "    if i==2:\n",
    "        continue\n",
    "    else:\n",
    "        sim=np.dot(W[2],W[i])/(norm(W[2])*norm(W[i]))\n",
    "        if sim>bestsim:\n",
    "            bestsim=sim\n",
    "            index=i\n",
    "\n",
    "bestsim,index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity of words\n",
    "\n",
    "just compare the columns from H with cosine (to which topic with which degree belongs the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H  # dimension 2 and 3: topic of 'awo' and 'bremerhafen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H[:,2]  # access to column 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(H[:,4],H[:,3])/(norm(H[:,4])*norm(H[:,3]))  # 'daewoo' and 'bremerhafen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(W,H),tfidf.todense()  # reconstruct orignial tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic 2: 1,6-10]\n",
    "\n",
    "H[2] # tranform 2. topic into 5 top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awo wedemeier landesverband geschäftsführer ute\n",
    "tfidf_feature_names[0:1] + tfidf_feature_names[5:6] +tfidf_feature_names[7:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic model for the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=500\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA\n",
    "#tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "#tf = tf_vectorizer.fit_transform(documents)\n",
    "#tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf_tfidf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(dtm_tfidf)\n",
    "#nmf2 = NMF(n_components=no_topics, solver='mu',random_state=1,beta_loss='kullback-leibler').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "#lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from sklearn.decomposition import NMF\n",
    "from __future__ import print_function\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)\n",
    "\n",
    "vis_nmf = pyLDAvis.sklearn.prepare(nmf_tfidf, dtm_tfidf, tfidf_vectorizer)\n",
    "vis_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Topic Modelling: Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "means: select the n most important sentences from a document\n",
    "\n",
    "most important:  sentences best representing the topic of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"lemmaNoun.tueba\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=5000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=3, max_features=no_features)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 50\n",
    "\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words=30\n",
    "\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=nmf.fit_transform(tfidf)\n",
    "H=nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tfidf[:1].toarray()  # tfidf representation of document 1\n",
    "\n",
    "topicIndex=getTopic(X,H,no_topics)  # get the topic of document 1 (its index)\n",
    "topicIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"doc.tueba\").read())   # doc=[sentence1,....] von segment 1\n",
    "\n",
    "# find the n best sentences (out of 33) of document 1 given the topic of document 1\n",
    "\n",
    "def bestSentencesOfTopic(doc,H,index,lines):\n",
    "    \"\"\" lines=number of sentences \n",
    "        index=topic index\n",
    "    \"\"\"\n",
    "    simIndexed={}\n",
    "    for i in range(0,lines):\n",
    "        X=tfidf_vectorizer.transform([doc[i]])\n",
    "        X=X[0].todense()\n",
    "        if norm(X)==0:\n",
    "            simIndexed[i]=0\n",
    "            continue\n",
    "        sim=np.dot(X,H[index])/(norm(X)*norm(H[index]))\n",
    "        simIndexed[i]=np.array(sim)[0].tolist()[0]  \n",
    "    return simIndexed\n",
    "    \n",
    "simIndexed= bestSentencesOfTopic(doc,H,topicIndex,33)\n",
    "\n",
    "simIndexed\n",
    "i=sorted([(value,key) for (key,value) in simIndexed.items()])\n",
    "i[-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences 2,7,10,19,20,22 are the best\n",
    "\n",
    "\n",
    "Summary: 6 sentences\n",
    "\n",
    "Flossen 165.000 Mark Sammelgelder für Flutopfer in ein Altenheim in Danzig ? \n",
    "Vorwurf Nummer 1 : 165.000 Mark aus der bundesweiten Geldsammlung für die Flutopfer in Südpolen seien über das Konto des Bremer Landesverbandes der AWO an die Caritas in Danzig geflossen , \" damit dort ein Altenheim gebaut wird \" .\n",
    "\" Wenn da was gebucht worden ist , dann ist das nicht in Ordnung \" - höchstens einen Buchungsfehler kann sie sich vorstellen . \n",
    "Die ehrenamtliche Landesvorsitzende Wedemeier weiß von diesem Vorgang nichts , \" ich kontrolliere solche Sachen doch nicht , das machen die hauptamtlichen Geschäftsführer . \" \n",
    "Kontrolliert werden die Geschäftsführer von den gewählten Revisoren des AWO-Landesverbandes , das sind Detlev Griesche und Karin Freudenthal . \n",
    "Da der Landesverband als Dachverband ohne hauptamtliches Personal nur einen \" ganz kleinen Haushalt \" hat ( Wedemeier ) , hätten Summen von Hapaq Lloyd oder 165.000 Mark schon auffallen müssen . \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text segment:\n",
    "\n",
    "Veruntreute die AWO Spendengeld ? \n",
    "Staatsanwaltschaft muß AWO-Konten prüfen\n",
    "Flossen 165.000 Mark Sammelgelder für Flutopfer in ein Altenheim in Danzig ? \n",
    "Landesvorsitzende Ute Wedemeier : Ein Buchungsfehler \n",
    "Im Januar hat die Arbeiterwohlfahrt Bremen ihren langjährigen Geschäftsführer Hans Taake fristlos entlassen , nun wird auch der Vorstand der Wohlfahrtsorganisation in den Fall hineingzogen . \n",
    "In einer anonymen Anzeige werden der Bremer Staatsanwaltschaft Details über dubiose finanzielle Transaktionen mitgeteilt . \n",
    "Verantwortlich , so das Schreiben einer Mitarbeiterin der AWO , sei die Landesvorsitzende Uter Wedemeier , die sich jetzt als \" Sauberfrau \" gebe , \" wo doch alle wissen , wie eng sie mit Taake zusammenhing \" . \n",
    "Vorwurf Nummer 1 : 165.000 Mark aus der bundesweiten Geldsammlung für die Flutopfer in Südpolen seien über das Konto des Bremer Landesverbandes der AWO an die Caritas in Danzig geflossen , \" damit dort ein Altenheim gebaut wird \" . \n",
    "Das Altenheim sei \" ein Prestigeobjekt von ihr und anderen \" . \n",
    "In der Tat sitzt Ute Wedemeier im Kuratorium für das Altenheim , eine derartige Umleitung von Geldern habe es aber nicht gegeben , sagt sie . \n",
    "\" Wenn da was gebucht worden ist , dann ist das nicht in Ordnung \" - höchstens einen Buchungsfehler kann sie sich vorstellen . \n",
    "Volker Tegeler , stellvertretender Geschäftsführer des Landesverbandes , sagt : \n",
    "\" Es gibt so eine Buchung . \" \n",
    "In einer internen Kontrolle nach der Kündigung von Taake sei dies aufgefallen , zur Aufklärung solle ein externer Wirtschaftsprüfer beauftragt werden . \n",
    "Verantwortlich für die Finanzen des Landesverbandes sei aber \" durchgehend Herr Taake \" gewesen , sagt Tegeler . \n",
    "Aufgefallen bei der internen Prüfung ist auch Vorwurf Nummer 2 : Die AWO hat sich für Seniorenreisen nach Mallorca von Hapaq Lloyd Provisionen zahlen lassen . \n",
    "Die seien auf ein Konto des Landesverbandes der AWO geflossen , weil sie dort vor einer Finanzamtsprüfung sicherer gewesen seien . \n",
    "Tegeler bestätigt den Vorgang der Provisionszahlungen , meint allerdings , es müsse ein \" Buchungsfehler \" gewesen sein . \n",
    "Die ehrenamtliche Landesvorsitzende Wedemeier weiß von diesem Vorgang nichts , \" ich kontrolliere solche Sachen doch nicht , das machen die hauptamtlichen Geschäftsführer . \" \n",
    "Kontrolliert werden die Geschäftsführer von den gewählten Revisoren des AWO-Landesverbandes , das sind Detlev Griesche und Karin Freudenthal . \n",
    "Freuden-thal wollte gestern nichts dazu sagen , ob bei ihren Prüfungen ihr etwas aufgefallen sei . \n",
    "Da der Landesverband als Dachverband ohne hauptamtliches Personal nur einen \" ganz kleinen Haushalt \" hat ( Wedemeier ) , hätten Summen von Hapaq Lloyd oder 165.000 Mark schon auffallen müssen . \n",
    "Vorwurf Nummer 3 : Die Landesvorsitzende Ute Wedemeier hatte auf AWO-Kosten ein Handy . \n",
    "\" Hier werden Beiträge von kleinen Leuten veraast , die von ehrenamtlichen Kassierern fünf Mark weise gesammelt werden \" , schreibt die anonyme AWO-Mitarbeiterin an die Staatsanwaltschaft . \n",
    "Obwohl Frau Wedemeier \" vor allem Privatgespräche über das Handy \" führe , würde alles von der AWO bezahlt . \n",
    "Ute Wedemeier hält es für \" selbstverständlich \" , daß sie als ehrenamtliche Vorsitzende ein dienstliches Handy hat . \n",
    "Insbesondere wegen ihrer Aktivitäten in Riga und Danzig müsse sie erreichbar sein und auch telefonieren können . \n",
    "Wieviel da monatlich fällig wird , weiß sie aber nicht - \" die Rechnung geht direkt an die AWO \" . \n",
    "Hintergrund der gegenseitigen Vorwürfe in der Arbeiterwohlfahrt sind offenbar scharfe Konkurrenzen zwischen Bremern und Bremerhavenern . \n",
    "Als es in dieser Woche um die Neubesetzung des ehrenamtlichen Geschäftsführer-Postens im Landesverbandes ging , da sind diese Differenzen wieder aufgebrochen . \n",
    "Lothar Koring , Bremerhavener AWO-Vorsitzender , wollte seinen Bremerhavener Geschäftsführer Volker Tegeler auch im Landesverband zum Geschäftsführer machen . \n",
    "Koring selbst hatte früher auch gegen Ute Wedemeier für den Landesvorsitz kandidiert . \n",
    "Gegen Tegeler sprach allerdings , daß noch ein staatsanwaltschaftliches Ermittlungsverfahren gegen ihn läuft . \n",
    "Und Koring war früher einmal in schiefes Licht geraten , weil er bei einer Prüfgesellschaft im Vorstand war , die die AWO , wo er Kreisvorsitzender ist , prüfte . \n",
    "Seine Position bei der Prüfgesellschaft mußte er damals niederlegen , den AWO-Posten nicht . \n",
    "K.W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parlament debattes\n",
    "\n",
    "Texts of C. Blocher, only words with initial caps are used (mostly nouns)\n",
    "* what are the topics\n",
    "* what are the most interesting statements of these topics\n",
    "\n",
    "* read the text \n",
    "* for each topic\n",
    "    * output the 3 most topic related sentences\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs,re\n",
    "\n",
    "origLine={}\n",
    "blocherDoc=[]\n",
    "docId=0\n",
    "with codecs.open('blocher.tsv',\"r\") as f:\n",
    "     for line in f:\n",
    "        party,speaker,text=line.strip().split('\\t')  \n",
    "        newtext= re.findall('[A-Z][a-zA-Züäö]+', text) # keep nouns as a list\n",
    "        if len(newtext) <3 :\n",
    "            continue\n",
    "        newline = ' '.join(newtext)  # produce a string\n",
    "        origLine[text]=newline       # index it\n",
    "        blocherDoc.append(newline)\n",
    "\n",
    "#blocherDoc #=blocherDoc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=5000\n",
    "no_topics = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=no_features)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(blocherDoc)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "vis_nmf = pyLDAvis.sklearn.prepare(nmf, tfidf, tfidf_vectorizer)\n",
    "vis_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W=nmf.fit_transform(tfidf)\n",
    "#H=nmf.components_\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSentencesOfTopic2(doc,H,index,lines):\n",
    "    \"\"\" lines=number of sentences \n",
    "        index=topic id\n",
    "    \"\"\"\n",
    "    simIndexed={}\n",
    "    for i in range(0,lines):\n",
    "        if norm(X)==0:\n",
    "            simIndexed[i]=0\n",
    "            continue\n",
    "        sim=np.dot(X,H[index])/(norm(X)*norm(H[index]))\n",
    "        simIndexed[i]=np.array(sim)[0].tolist()[0]  \n",
    "    return simIndexed\n",
    "\n",
    "\n",
    "def getTopic2(X,H,topicLen):\n",
    "    \"\"\" which topic is most similar to X (a tfidf representation of a document)\"\"\"\n",
    "    bestsim=0\n",
    "    index=0\n",
    "    for i in range(0,topicLen-1):\n",
    "        sim=np.dot(X,H[i])/(norm(X)*norm(H[i]))\n",
    "        if sim>bestsim:\n",
    "            bestsim=sim\n",
    "            index=i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=nmf.fit_transform(tfidf)\n",
    "H=nmf.components_\n",
    "\n",
    "docId=0\n",
    "sentOfTopic={}\n",
    "    \n",
    "for X in tfidf:\n",
    "    X=X[0].toarray()    \n",
    "    index=getTopic(X,H,no_topics)  \n",
    "    sim=np.dot(X,H[index])/(norm(X)*norm(H[index]))\n",
    "    if index in sentOfTopic:\n",
    "        sentOfTopic[index].append(docId)\n",
    "    else:\n",
    "        sentOfTopic[index]=[docId]\n",
    "    docId+=1\n",
    "    \n",
    "for t in sentOfTopic:\n",
    "    simIndexed={}\n",
    "    bestsim=0\n",
    "    if t==1:\n",
    "        print(sentOfTopic[t])\n",
    "        break\n",
    "    for d in sentOfTopic[t]:\n",
    "        if d==1185:\n",
    "            print(t)\n",
    "            \n",
    "        X=tfidf[d]\n",
    "        X=X[0].todense()\n",
    "        sim=np.dot(X,H[t])/(norm(X)*norm(H[t]))\n",
    "        sim=np.array(sim)[0].tolist()[0]\n",
    "        if sim > bestsim:\n",
    "            bestsim=sim\n",
    "            doc=d\n",
    "    print(t,doc,bestsim)\n",
    "    print(blocherDoc[doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svp\n",
    "origLine={}\n",
    "svpDoc=[]\n",
    "docId=0\n",
    "with codecs.open('svp.tsv',\"r\") as f:\n",
    "     for line in f:\n",
    "        party,speaker,text=line.strip().split('\\t')  \n",
    "        newtext= re.findall('[A-Z][a-zA-Züäö]+', text) # keep nouns as a list\n",
    "        newline = ' '.join(newtext)  # produce a string\n",
    "        if len(newtext) <3 :\n",
    "            continue\n",
    "        origLine[text]=newline      # index it\n",
    "        svpDoc.append(newline)\n",
    "\n",
    "svpDoc=removeDocs(blocherDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=5000\n",
    "no_topics = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=no_features)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(svpDoc)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "W=nmf.fit_transform(tfidf)\n",
    "H=nmf.components_\n",
    "\n",
    "vis_nmf = pyLDAvis.sklearn.prepare(nmf, tfidf, tfidf_vectorizer)\n",
    "vis_nmf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp\n",
    "origLine={}\n",
    "spDoc=[]\n",
    "docId=0\n",
    "with codecs.open('svp.tsv',\"r\") as f:\n",
    "     for line in f:\n",
    "        party,speaker,text=line.strip().split('\\t')  \n",
    "        newtext= re.findall('[A-Z][a-zA-Züäö]+', text) # keep nouns as a list\n",
    "        newline = ' '.join(newtext)  # produce a string\n",
    "        if len(newtext) <3 :\n",
    "            continue\n",
    "        origLine[text]=newline      # index it\n",
    "        spDoc.append(newline)\n",
    "\n",
    "spDoc=removeDocs(blocherDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features=5000\n",
    "no_topics = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=no_features)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(blocherDoc)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "W=nmf.fit_transform(tfidf)\n",
    "H=nmf.components_\n",
    "\n",
    "vis_nmf = pyLDAvis.sklearn.prepare(nmf, tfidf, tfidf_vectorizer)\n",
    "vis_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import NMF\n",
    "from __future__ import print_function\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)\n",
    "\n",
    "vis_nmf = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)\n",
    "vis_nmf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
